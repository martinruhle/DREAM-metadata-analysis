---
title: "Preterm Prediction Model using Tidymodels"
author: "Martin Ruhle"
date: "2024-10-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document describes the process of building a prediction model for preterm births using the tidymodels package in R. Predicting preterm births is complex, often involving various demographic, clinical, and lifestyle factors. In this first model iteration, we use only one predictor (`NIH.Racial.Category`), representing racial categorization, as a starting point. This approach allows us to establish a baseline model before adding more features in future iterations. The model chosen for this analysis is logistic regression, suitable for binary classification tasks like predicting preterm birth (`was_preterm`), where we aim to classify births as preterm or non-preterm.

For evaluation, we look at metrics like accuracy, sensitivity, and specificity. This preliminary model allows us to assess whether there is any predictive signal using just the racial category, which will inform the addition of other predictors in a more comprehensive model later.


```{r warning=FALSE, message=FALSE}
# Load necessary libraries
library(tidymodels)
```

## Data Loading and Preprocessing

We load the metadata file and filter the columns needed for modeling and convert the outcome variables `was_preterm` and `was_early_preterm` to factors, which is required for classification models.

```{r eval=FALSE}
# Read the data
metadata <- read.csv("metadata.csv")
```
```{r echo=FALSE}
metadata <- read.csv("C:/Users/User/Documents/DREAM/Git_hub_repository/data/metadata.csv")
```
```{r}
# Preprocessing: Use only the necessary columns for the model, filtering out other data. 
# Encode was_preterm and was_early_preterm as factors for classification.
metadata <- metadata %>%
  select(NIH.Racial.Category, was_preterm, was_early_preterm) %>%
  mutate(across(c(was_preterm, was_early_preterm), as.factor))

# Checking for missing values
colSums(is.na(metadata))
```

## Split the Data
We split the data into training and testing sets with an 80-20 split, stratified by the `was_preterm` variable to ensure balanced classes in both sets.

```{r}
# Split the Data: Split the data into training and testing sets.
# ´group_initial_split´ could be used considering participants as groups 
set.seed(123)
data_split <- initial_split(metadata, prop = 0.8, strata = was_preterm)
train_data <- training(data_split)
test_data <- testing(data_split)
```

## Creating Recipes
We set up recipes to preprocess the predictor variable `NIH.Racial.Category` by converting it to dummy variables for modeling. Separate recipes are created for predicting `was_preterm` and `was_early_preterm`.

```{r}
# Create a Recipe: Set up a recipe for preprocessing, 
# such as converting categorical variables to dummy variables.
preterm_recipe <- recipe(was_preterm ~ NIH.Racial.Category, data = train_data) %>%
  step_dummy(all_nominal_predictors())

early_preterm_recipe <- recipe(was_early_preterm ~ NIH.Racial.Category, data = train_data) %>%
  step_dummy(all_nominal_predictors())
```

## Model Specification
We specify a logistic regression model, which is suitable for binary classification, and set it to use the glm engine for estimation.

```{r}
# Specify the Model: Choose a classification model, 
# like logistic regression or decision tree, and specify the model parameters.
model_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
```

## Creating Workflows
We combine the recipes and model specifications into workflows, which will streamline the training and prediction process.

```{r}
# Create a Workflow: Combine the recipe and model specification.
workflow_preterm <- workflow() %>%
  add_recipe(preterm_recipe) %>%
  add_model(model_spec)

workflow_early_preterm <- workflow() %>%
  add_recipe(early_preterm_recipe) %>%
  add_model(model_spec)
```

## Model Fitting
We fit the workflows to the training data for each target variable, allowing us to make predictions and evaluate model performance.

```{r}
# Fit the Model: Fit the model on the training data.
preterm_fit <- fit(workflow_preterm, data = train_data)
early_preterm_fit <- fit(workflow_early_preterm, data = train_data)
```

## Model Evaluation
### Making Predictions
We use the test data to generate predictions and evaluate the model. These predictions are then joined with the test data for calculating various performance metrics.

```{r}
# Evaluate the Models: Use the testing set to evaluate models performance.
predictions <- predict(preterm_fit, test_data) %>%
  bind_cols(test_data)

predictions_early <- predict(early_preterm_fit, test_data) %>%
  bind_cols(test_data)
```

## Confusion Matrix
The confusion matrix summarizes the model's prediction results by showing the number of true positive, false positive, true negative, and false negative predictions.

```{r}
# Calculate the confusion matrices
conf_matrix <- predictions %>%
  conf_mat(truth = was_preterm, estimate = .pred_class)

conf_matrix_early <- predictions_early %>%
  conf_mat(truth = was_early_preterm, estimate = .pred_class)

# Print the confusion matrices
print(conf_matrix)
print(conf_matrix_early)
```

## Calculating Model Metrics
We create a set of metrics to assess the model's performance, including accuracy, kappa, sensitivity, specificity, positive and negative predictive values, detection prevalence, Matthews correlation coefficient (MCC), and balanced accuracy.

```{r}
# Define the metric set with yardstick functions only
metrics <- metric_set(accuracy, kap, sensitivity, specificity,
                      ppv, npv, detection_prevalence, mcc, bal_accuracy)

# Calculate metrics using the correct functions
model_metrics <- predictions %>%
  metrics(truth = was_preterm, estimate = .pred_class)

model_metrics_early <- predictions_early %>%
  metrics(truth = was_early_preterm, estimate = .pred_class)

# Print the model metrics
print(model_metrics)
print(model_metrics_early)
```

*<u>Accuracy</u>: This is the overall proportion of correctly predicted samples (both true positives and true negatives) out of the total number of samples. Here, an accuracy of 0.659 means that about 65.9% of predictions matched the true labels. For early preterm, accuracy is higher at 88.4%.

*<u>Kappa (kap)</u>: Cohen’s Kappa measures the agreement between the predictions and actual values, adjusted for the likelihood of agreement by chance. Values range from -1 to 1, where 1 is perfect agreement, 0 is chance-level, and negative values indicate worse-than-chance predictions. Here, a kappa of 0.116 suggests low agreement between predictions and actual values beyond what would be expected by chance.

*<u>Sensitivity</u>: Also called the true positive rate or recall, sensitivity measures the proportion of actual positive cases that were correctly identified. In this case, 0.926 sensitivity indicates that 92.6% of true preterm cases were correctly identified by the model.

*<u>Specificity</u>: This measures the proportion of actual negative cases (non-preterm) that were correctly identified. A specificity of 0.172 here means that only 17.2% of non-preterm cases were correctly classified, indicating the model is not very effective at predicting the "FALSE" class (non-preterm).

*<u>Positive Predictive Value (PPV)</u>: Also known as precision, PPV is the proportion of positive predictions that are actually correct. A PPV of 0.671 means that 67.1% of cases predicted as preterm were indeed preterm.

*<u>Negative Predictive Value (NPV)</u>: This is the proportion of negative predictions (non-preterm) that are actually correct. An NPV of 0.561 means that 56.1% of cases predicted as non-preterm were actually non-preterm.

*<u>Detection Prevalence</u>: This metric shows the proportion of samples predicted as positive (preterm) by the model, regardless of whether they are true positives. Here, a detection prevalence of 0.892 indicates that 89.2% of samples were predicted as preterm.

*<u>Matthews Correlation Coefficient (MCC)</u>: MCC is a balanced measure that considers true positives, true negatives, false positives, and false negatives, with values ranging from -1 to 1. A value closer to 1 indicates better predictive performance across both classes. The MCC of 0.151 here suggests a weak correlation between predictions and actual outcomes.

*<u>Balanced Accuracy</u>: This is the average of sensitivity and specificity, providing an adjusted measure of accuracy that accounts for imbalanced class distributions. A balanced accuracy of 0.549 indicates that the model performs slightly better than random chance (0.5) across both classes.

# No Information Rate and P-Value Calculation
We calculate the No Information Rate (NIR), which reflects the accuracy a model would achieve by always predicting the majority class, and assess the statistical significance of our model’s performance compared to NIR.

```{r}
# Calculate the No Information Rate (NIR)
nir <- max(prop.table(table(test_data$was_preterm)))
nir_early <- max(prop.table(table(test_data$was_early_preterm)))

# Calculate accuracy of the models
accuracy_metric <- model_metrics %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

accuracy_metric_early <- model_metrics_early %>%
  filter(.metric == "accuracy") %>%
  pull(.estimate)

# Perform a binomial test to get the P-value for Acc > NIR
p_value_acc_gt_nir <- binom.test(
  x = round(accuracy_metric * nrow(test_data)), # number of correct predictions
  n = nrow(test_data),                          # total number of predictions
  p = nir                                       # probability under the null (NIR)
)$p.value

p_value_acc_gt_nir_early <- binom.test(
  x = round(accuracy_metric_early * nrow(test_data)), 
  n = nrow(test_data),                          
  p = nir                                       
)$p.value

# Extract values from the confusion matrix
false_neg <- conf_matrix$table[2, 1]  # True class was FALSE, predicted TRUE
false_pos <- conf_matrix$table[1, 2]  # True class was TRUE, predicted FALSE
false_neg_early <- conf_matrix_early$table[2, 1]  
false_pos_early <- conf_matrix_early$table[1, 2]  

# Perform McNemar's test
mcnemar_test <- mcnemar.test(matrix(c(conf_matrix$table[1, 1], false_pos, 
                                      false_neg, conf_matrix$table[2, 2]), 
                                    nrow = 2))

mcnemar_test_early <- mcnemar.test(matrix(c(
  conf_matrix_early$table[1,1],false_pos_early,
  false_neg_early, conf_matrix_early$table[2, 2]), 
                                    nrow = 2))

# Display results
# Preterm
nir
p_value_acc_gt_nir
mcnemar_test$p.value
# Early preterm
nir_early
p_value_acc_gt_nir_early
mcnemar_test_early$p.value
```

*<u>No Information Rate (NIR)</u>: NIR represents the accuracy expected by always predicting the majority class, providing a baseline for model comparison. Calculating it is essential as it shows if the model’s accuracy significantly exceeds this baseline.

*<u>Binomial Test for Accuracy > NIR</u>: This test checks if the observed accuracy of the model is statistically greater than the NIR, providing a p-value. If significant, it confirms that the model performs better than simply predicting the most frequent class.

*<u>McNemar’s Test</u>: Used to assess if there is a significant difference between two classifiers on paired nominal data. In this case, McNemar's test checks the symmetry of prediction errors (false positives vs. false negatives) for each confusion matrix. The p-values here indicate significant differences, suggesting imbalance between the model’s predictions of false positives and false negatives, which could imply model calibration issues.

## Results
The NIR for preterm predictions is 0.645, meaning a simple majority-class model would be correct 64.5% of the time. The model’s accuracy of 65.9%, with a p-value of 0.6286, suggests this accuracy is not significantly better than the NIR. The NIR for early preterm is 0.884, aligning exactly with the model’s accuracy of 88.4%. The binomial test p-value of 9.07e-26 doesn't make sense because the NIR and accuracy are equal.

McNemar’s test for preterm shows a very low p-value (5.489e-16), highlighting significant discrepancies in false positives and false negatives, possibly due to the model over-predicting the majority class.
For early preterm the McNemar’s test p-value (9.02e-11) also shows significant asymmetry in misclassifications, indicating potential issues in capturing early preterm patterns.

# Conclusion
This preliminary model provides a starting point for predicting preterm births using only NIH.Racial.Category as a predictor. While it demonstrates some predictive ability, the low specificity and MCC suggest that the model is limited in its capacity to generalize, particularly when distinguishing non-preterm cases. The relatively high sensitivity indicates that the model captures a significant portion of preterm cases but at the cost of more false positives.

Future models will incorporate additional clinical and demographic predictors to improve classification accuracy and reduce false positive rates, ultimately providing a more comprehensive predictive tool for preterm births.